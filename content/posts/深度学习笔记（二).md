---
date: 2025-01-26T16:00:54+08:00
title: 深度学习笔记（二)
description: 卷积神经网络
featured_image: /images/book.png
images:
  - /images/book.png
tags:
  - tutorial
  - DL
categories: Tutorials
---
## 卷积神经网络

卷积神经网络（Convolutional Neural Network, **CNN**）是一种专为处理网格结构数据（如图像和时间序列）设计的深度学习模型。它是一类前馈神经网络，包含专门用于提取数据空间信息的卷积操作，其主要特点是通过局部连接和参数共享，有效地提取输入数据的局部特征，同时降低模型的参数量。

---

### 核心组成部分

1. **输入层**
    
    - 接受原始输入数据，例如一张图像（RGB 图像是一个三维张量：宽度 × 高度 × 通道数）。
2. **卷积层（Convolutional Layer）**
    
    - 核心功能是提取输入数据的局部特征，例如边缘、纹理等。通过卷积核（filters/kernels）在输入数据上滑动，计算内积，生成特征图（Feature Map）。
    - 特点：
        - **局部感受野**：每个神经元只感知输入的一小部分（局部区域）。
        - **参数共享**：相同的卷积核在整个输入上应用，从而减少参数数量。
3. **激活函数（Activation Function）**
    
    - 常用 ReLU（Rectified Linear Unit）作为非线性激活函数，增加模型的表达能力。
4. **池化层（Pooling Layer）**
    
    - 用于下采样特征图，减少数据维度，同时保留关键特征，减小计算复杂度并降低过拟合风险。
    - 常用方法：**最大池化（Max Pooling）** 和 **平均池化（Average Pooling）**。
5. **全连接层（Fully Connected Layer）**
    
    - 将卷积和池化层输出的高维特征展平为一维向量，并连接到输出层。通过加权求和和激活函数计算最终输出。
6. **输出层**
    
    - 根据任务要求输出结果，例如分类任务的概率分布（通过 softmax）或回归任务的实值。

---

### 特点

1. **局部连接**：每个神经元只关注输入的一部分（称为感受野），非常适合处理图像等具有局部相关性的结构化数据。
2. **参数共享**：卷积核在整个输入数据上重复使用，大幅降低参数量，提高计算效率。
3. **空间不变性**：通过卷积和池化操作，CNN 可以很好地捕捉输入数据的空间特征，例如边缘、纹理和形状等。

---

### 工作流程（以图像分类为例）

1. **输入**：输入一张图像，假设大小为 32×32×332 \times 32 \times 3（宽、长、RGB 通道）。
2. **卷积**：用多个卷积核扫描图像，提取低层特征（如边缘）。
3. **激活**：应用 ReLU 非线性激活函数。
4. **池化**：对卷积特征进行下采样，减少数据维度。
5. **多次重复卷积和池化**：提取更深层次特征（如纹理、形状等）。
6. **全连接**：将提取的高层特征展平，输入全连接层，用于最终的分类。
7. **输出**：通过 softmax 函数输出每个类别的概率。

---

### 优点

1. **自动特征提取**：无需手工设计特征，CNN 能自动学习有效的特征表示。
2. **减少参数量**：通过局部连接和参数共享，参数量远少于传统的全连接网络。
3. **适用于图像处理**：特别适合图像分类、目标检测、图像生成等任务。

---

### 应用场景

1. **图像处理**：图像分类、目标检测、图像分割（如识别猫狗图片）。
2. **自然语言处理**：文本分类、句法分析。
3. **医疗影像分析**：癌症检测、医学图像分割。
4. **自动驾驶**：目标检测（如行人识别、车道检测）。
5. **其他领域**：推荐系统、时间序列数据分析等。

---

CNN 是深度学习的核心技术之一，它在视觉领域取得了显著的成功。通过提取多层次的特征，CNN 能够从简单的边缘信息到复杂的全局信息，逐步构建对输入数据的理解。

## C语言示例

要实现一个包含完整架构的卷积神经网络（输入层、卷积层、池化层、全连接层和输出层）的代码，虽然复杂度会有所增加，但我们可以通过简化网络结构，构建一个基本的 CNN 框架来实现。

以下是一个简单的例子，它会：

1. **输入层**：定义输入图像（一个 4x4 矩阵）。
2. **卷积层**：用一个 3x3 的卷积核计算特征图。
3. **池化层**：用最大池化（2x2）下采样特征图。
4. **全连接层**：将池化后的数据展平成向量并通过权重连接。
5. **输出层**：输出一个二维分类结果（假设为二分类任务）。

---

### 完整代码

```c
#include <stdio.h>
#include <math.h>

#define INPUT_SIZE 4
#define KERNEL_SIZE 3
#define CONV_OUTPUT_SIZE (INPUT_SIZE - KERNEL_SIZE + 1)
#define POOL_SIZE 2
#define POOL_OUTPUT_SIZE (CONV_OUTPUT_SIZE / POOL_SIZE)
#define FC_INPUT_SIZE (POOL_OUTPUT_SIZE * POOL_OUTPUT_SIZE)
#define FC_OUTPUT_SIZE 2 // 假设输出为2类

// 激活函数（ReLU）
float relu(float x) {
    return x > 0 ? x : 0;
}

// 打印一维矩阵
void printVector(float vector[], int size) {
    for (int i = 0; i < size; i++) {
        printf("%.2f ", vector[i]);
    }
    printf("\n");
}

// 打印二维矩阵
void printMatrix(float matrix[][CONV_OUTPUT_SIZE], int rows, int cols) {
    for (int i = 0; i < rows; i++) {
        for (int j = 0; j < cols; j++) {
            printf("%.2f ", matrix[i][j]);
        }
        printf("\n");
    }
}

// 卷积层操作
void convolution2D(float input[INPUT_SIZE][INPUT_SIZE], 
                   float kernel[KERNEL_SIZE][KERNEL_SIZE], 
                   float output[CONV_OUTPUT_SIZE][CONV_OUTPUT_SIZE]) {
    for (int i = 0; i < CONV_OUTPUT_SIZE; i++) {
        for (int j = 0; j < CONV_OUTPUT_SIZE; j++) {
            float sum = 0.0;
            for (int ki = 0; ki < KERNEL_SIZE; ki++) {
                for (int kj = 0; kj < KERNEL_SIZE; kj++) {
                    sum += input[i + ki][j + kj] * kernel[ki][kj];
                }
            }
            output[i][j] = relu(sum); // 应用 ReLU 激活
        }
    }
}

// 最大池化层操作
void maxPooling2D(float input[CONV_OUTPUT_SIZE][CONV_OUTPUT_SIZE], 
                  float output[POOL_OUTPUT_SIZE][POOL_OUTPUT_SIZE]) {
    for (int i = 0; i < POOL_OUTPUT_SIZE; i++) {
        for (int j = 0; j < POOL_OUTPUT_SIZE; j++) {
            float maxVal = -INFINITY;
            for (int pi = 0; pi < POOL_SIZE; pi++) {
                for (int pj = 0; pj < POOL_SIZE; pj++) {
                    float val = input[i * POOL_SIZE + pi][j * POOL_SIZE + pj];
                    if (val > maxVal) {
                        maxVal = val;
                    }
                }
            }
            output[i][j] = maxVal;
        }
    }
}

// 全连接层操作
void fullyConnected(float input[], float weights[][FC_OUTPUT_SIZE], float biases[], float output[]) {
    for (int i = 0; i < FC_OUTPUT_SIZE; i++) {
        float sum = 0.0;
        for (int j = 0; j < FC_INPUT_SIZE; j++) {
            sum += input[j] * weights[j][i];
        }
        output[i] = relu(sum + biases[i]); // 应用 ReLU 激活
    }
}

int main() {
    // 输入层（4x4 图像）
    float input[INPUT_SIZE][INPUT_SIZE] = {
        {1, 2, 3, 0},
        {4, 5, 6, 1},
        {7, 8, 9, 2},
        {0, 1, 2, 3}
    };

    // 卷积核（3x3）
    float kernel[KERNEL_SIZE][KERNEL_SIZE] = {
        {1, 0, -1},
        {1, 0, -1},
        {1, 0, -1}
    };

    // 卷积层输出
    float convOutput[CONV_OUTPUT_SIZE][CONV_OUTPUT_SIZE] = {0};
    convolution2D(input, kernel, convOutput);

    printf("Convolution Output:\n");
    printMatrix(convOutput, CONV_OUTPUT_SIZE, CONV_OUTPUT_SIZE);

    // 池化层输出
    float poolOutput[POOL_OUTPUT_SIZE][POOL_OUTPUT_SIZE] = {0};
    maxPooling2D(convOutput, poolOutput);

    printf("\nPooling Output:\n");
    printMatrix(poolOutput, POOL_OUTPUT_SIZE, POOL_OUTPUT_SIZE);

    // 展平池化层输出
    float flatInput[FC_INPUT_SIZE] = {0};
    int index = 0;
    for (int i = 0; i < POOL_OUTPUT_SIZE; i++) {
        for (int j = 0; j < POOL_OUTPUT_SIZE; j++) {
            flatInput[index++] = poolOutput[i][j];
        }
    }

    // 全连接层权重和偏置
    float fcWeights[FC_INPUT_SIZE][FC_OUTPUT_SIZE] = {
        {0.1, 0.2}, {0.3, 0.4}, {0.5, 0.6}, {0.7, 0.8},
        {0.9, 1.0}, {1.1, 1.2}, {1.3, 1.4}, {1.5, 1.6},
        {1.7, 1.8}
    };
    float fcBiases[FC_OUTPUT_SIZE] = {0.5, -0.5};

    // 全连接层输出
    float fcOutput[FC_OUTPUT_SIZE] = {0};
    fullyConnected(flatInput, fcWeights, fcBiases, fcOutput);

    printf("\nFully Connected Output:\n");
    printVector(fcOutput, FC_OUTPUT_SIZE);

    return 0;
}
```

---

### 核心解释

1. **卷积层**：
    
    - 使用 3x3 卷积核。
    - 计算结果并通过 ReLU 激活函数进行非线性处理。
2. **池化层**：
    
    - 采用 2x2 最大池化，减小特征图的大小。
3. **全连接层**：
    
    - 展平池化层的输出（从 2D 转为 1D）。
    - 使用简单的权重和偏置实现全连接操作。
4. **输出层**：
    
    - 假设输出为二维（用于分类，比如二分类问题）。

---

### 输出示例

运行结果可能如下：

```
Convolution Output:
12.00 15.00 
6.00 6.00 

Pooling Output:
15.00 

Fully Connected Output:
33.00 34.00 
```

---

这段代码实现了最基本的 CNN 结构，适合初学者理解 CNN 的各个模块如何协作。如果需要进一步扩展，可以加入更多卷积层、池化层或使用优化方法（如反向传播）进行训练。